# 收集的一些有关注意力的文献(不断填充)
## 1. SE

[SE+CBAM-pt-代码](https://blog.csdn.net/weixin_42907473/article/details/106525668)

## 2. BAM&CBAM

[论文](https://arxiv.org/abs/1807.06521)

[代码实现](https://github.com/Jongchan/attention-module)

## 3. SK

[论文](https://arxiv.org/abs/1903.06586)

[解析](https://zhuanlan.zhihu.com/p/59690223)

[代码](https://github.com/implus/SKNet)

## 4. ECA

[论文](https://arxiv.org/pdf/1910.03151.pdf)

[解析](https://zhuanlan.zhihu.com/p/86100279)

[代码](https://github.com/BangguWu/ECANet)

## 5. GCT

[论文](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Gated_Channel_Transformation_for_Visual_Recognition_CVPR_2020_paper.pdf)

[解析](https://zhuanlan.zhihu.com/p/164902586)

[代码](https://github.com/z-x-yang/GCT)

## 6. DeepSquare

[论文](https://arxiv.org/ftp/arxiv/papers/1906/1906.04979.pdf)

[解析](https://zhuanlan.zhihu.com/p/161971934)


## 7. SRM
[论文](https://arxiv.org/pdf/1903.10829.pdf)

[解析](https://zhuanlan.zhihu.com/p/96717845)

[代码](https://github.com/hyunjaelee410/style-based-recalibration-module)
